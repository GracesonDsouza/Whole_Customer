# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UtaIDt9rd3cvf_cM2EuUxvyqIgQFXWwH
"""

# app.py
import streamlit as st
import pandas as pd
import numpy as np
import pickle
from io import BytesIO

st.set_page_config(page_title="Wholesale Customer Segmentation", layout="wide")

st.title("Wholesale Customer Segmentation (K-Means)")
st.caption("Predict customer segment based on annual spending across product categories.")

# -----------------------
# Load Pickle Artifacts
# -----------------------
@st.cache_resource
def load_artifacts(pkl_path: str):
    with open(pkl_path, "rb") as f:
        artifacts = pickle.load(f)
    return artifacts

# Sidebar: model file
st.sidebar.header("Model")
pkl_file = st.sidebar.text_input("Pickle file path", value="wholesale_customer_segmentation.pkl")

try:
    artifacts = load_artifacts(pkl_file)
    kmeans = artifacts["kmeans"]
    scaler = artifacts["scaler"]
    features = artifacts["features"]  # expected column order
    k_final = artifacts.get("k", None)
    st.sidebar.success(f"Loaded model ✅ (k={k_final})" if k_final else "Loaded model ✅")
except Exception as e:
    st.sidebar.error("Could not load the pickle file.")
    st.sidebar.exception(e)
    st.stop()

# -----------------------
# Helper: single prediction
# -----------------------
def predict_cluster_from_inputs(values_dict: dict) -> int:
    # Ensure correct column order
    X_new = pd.DataFrame([values_dict])[features]

    # Same preprocessing as notebook: log1p then scaling
    X_log = np.log1p(X_new)
    X_scaled = scaler.transform(X_log)

    return int(kmeans.predict(X_scaled)[0])

# -----------------------
# Layout tabs
# -----------------------
tab1, tab2 = st.tabs(["Single Customer", "Batch (CSV Upload)"])

# =======================
# TAB 1: Single Customer
# =======================
with tab1:
    st.subheader("Predict Cluster for a Single Customer")

    colA, colB, colC = st.columns(3)

    # Reasonable defaults; you can adjust
    defaults = {
        "Fresh": 5000,
        "Milk": 3000,
        "Grocery": 4000,
        "Frozen": 1500,
        "Detergents_Paper": 1000,
        "Delicassen": 800,
    }

    inputs = {}

    # Use number inputs (better than sliders for large ranges)
    with colA:
        inputs["Fresh"] = st.number_input("Fresh (annual spending)", min_value=0.0, value=float(defaults["Fresh"]), step=100.0)
        inputs["Milk"] = st.number_input("Milk (annual spending)", min_value=0.0, value=float(defaults["Milk"]), step=100.0)

    with colB:
        inputs["Grocery"] = st.number_input("Grocery (annual spending)", min_value=0.0, value=float(defaults["Grocery"]), step=100.0)
        inputs["Frozen"] = st.number_input("Frozen (annual spending)", min_value=0.0, value=float(defaults["Frozen"]), step=100.0)

    with colC:
        inputs["Detergents_Paper"] = st.number_input("Detergents_Paper (annual spending)", min_value=0.0, value=float(defaults["Detergents_Paper"]), step=100.0)
        inputs["Delicassen"] = st.number_input("Delicassen (annual spending)", min_value=0.0, value=float(defaults["Delicassen"]), step=100.0)

    st.write("**Input Preview**")
    st.dataframe(pd.DataFrame([inputs])[features], use_container_width=True)

    if st.button("Predict Cluster", type="primary"):
        try:
            cluster_id = predict_cluster_from_inputs(inputs)
            st.success(f"Predicted Cluster: **{cluster_id}**")

            # Simple “segment hint” (generic, safe)
            st.info(
                "Tip: Next, interpret this cluster by comparing its centroid/means vs other clusters "
                "(e.g., high Grocery+Detergents may indicate retail-style buyers)."
            )
        except Exception as e:
            st.error("Prediction failed.")
            st.exception(e)

# =======================
# TAB 2: Batch Prediction
# =======================
with tab2:
    st.subheader("Batch Cluster Prediction (Upload CSV)")
    st.caption(f"CSV must contain these columns: {', '.join(features)}")

    uploaded_csv = st.file_uploader("Upload your CSV file", type=["csv"])

    if uploaded_csv is not None:
        try:
            df = pd.read_csv(uploaded_csv)

            missing = [c for c in features if c not in df.columns]
            if missing:
                st.error(f"Missing required columns: {missing}")
                st.stop()

            df_features = df[features].copy()

            # Preprocess like notebook: log1p then scale
            X_log = np.log1p(df_features)
            X_scaled = scaler.transform(X_log)

            preds = kmeans.predict(X_scaled).astype(int)
            df_out = df.copy()
            df_out["Cluster"] = preds

            st.success("Batch prediction completed ✅")
            st.write("Preview (first 20 rows):")
            st.dataframe(df_out.head(20), use_container_width=True)

            # Quick analytics if user wants
            with st.expander("Show cluster summary"):
                st.write("Cluster counts:")
                st.dataframe(df_out["Cluster"].value_counts().sort_index().rename_axis("Cluster").to_frame("Count"),
                             use_container_width=True)

                st.write("Cluster means (spending columns):")
                st.dataframe(df_out.groupby("Cluster")[features].mean(), use_container_width=True)

            # Download results
            csv_bytes = df_out.to_csv(index=False).encode("utf-8")
            st.download_button(
                "Download Results CSV",
                data=csv_bytes,
                file_name="wholesale_customers_with_clusters.csv",
                mime="text/csv"
            )

        except Exception as e:
            st.error("Could not process the uploaded file.")
            st.exception(e)

# -----------------------
# Footer
# -----------------------
st.markdown("---")
st.caption("Model pipeline follows: log1p transform → StandardScaler → K-Means clustering. :contentReference[oaicite:1]{index=1}")