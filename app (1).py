# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kN4siXEd2R7xeN3hUuawVQ7QUQIkDHy
"""

# app.py
import streamlit as st
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt

from sklearn.decomposition import PCA

st.set_page_config(page_title="Wholesale Customer Segmentation", layout="wide")

st.title("Wholesale Customer Segmentation (K-Means)")
st.caption("Predict customer segment based on annual spending across product categories.")

# -----------------------
# Load Pickle Artifacts
# -----------------------
@st.cache_resource
def load_artifacts(pkl_path: str):
    with open(pkl_path, "rb") as f:
        artifacts = pickle.load(f)
    return artifacts

st.sidebar.header("Model")
pkl_file = st.sidebar.text_input("Pickle file path", value="wholesale_customer_segmentation.pkl")

try:
    artifacts = load_artifacts(pkl_file)
    kmeans = artifacts["kmeans"]
    scaler = artifacts["scaler"]
    features = artifacts["features"]  # column order
    k_final = artifacts.get("k", None)
    st.sidebar.success(f"Loaded model ✅ (k={k_final})" if k_final else "Loaded model ✅")
except Exception as e:
    st.sidebar.error("Could not load the pickle file.")
    st.sidebar.exception(e)
    st.stop()

# -----------------------
# Cluster Meaning (Edit these to match your final cluster story)
# -----------------------
CLUSTER_INFO = {
    0: {
        "name": "High-Volume Retail / Bulk Buyers",
        "what": "High spend on Grocery + Detergents_Paper; bulk essentials-focused.",
        "action": "Offer bulk pricing, stable replenishment cycles, contract-based deals."
    },
    1: {
        "name": "HoReCa / Fresh-Focused Customers",
        "what": "High spend on Fresh (often with Frozen); perishable and quality-driven.",
        "action": "Prioritize quick delivery, freshness SLAs, relationship-based selling."
    },
    2: {
        "name": "Balanced / Medium-Value Buyers",
        "what": "Moderate spending across most categories; diversified buying pattern.",
        "action": "Cross-sell bundles, personalized promos, upgrade to higher-value mix."
    },
    3: {
        "name": "Low-Volume / Occasional Buyers",
        "what": "Low spending across categories; infrequent orders.",
        "action": "Serve cost-efficiently (automation), focus sales effort on higher-value clusters."
    }
}

def explain_cluster(cluster_id: int):
    info = CLUSTER_INFO.get(cluster_id, None)
    if not info:
        st.info(f"Cluster {cluster_id}: No description added yet.")
        return

    st.subheader(f"Cluster {cluster_id}: {info['name']}")
    st.write(f"**What it represents:** {info['what']}")
    st.write(f"**Recommended action:** {info['action']}")

# -----------------------
# Helper: single prediction
# -----------------------
def preprocess(df_features: pd.DataFrame) -> np.ndarray:
    # log1p then scale (same as notebook)
    X_log = np.log1p(df_features)
    X_scaled = scaler.transform(X_log)
    return X_scaled

def predict_single(values_dict: dict) -> int:
    df_new = pd.DataFrame([values_dict])[features]
    X_scaled = preprocess(df_new)
    return int(kmeans.predict(X_scaled)[0])

# -----------------------
# Tabs
# -----------------------
tab1, tab2 = st.tabs(["Single Customer", "Batch (CSV Upload)"])

# =======================
# TAB 1: Single Customer
# =======================
with tab1:
    st.subheader("Predict Cluster for a Single Customer")

    colA, colB, colC = st.columns(3)

    defaults = {
        "Fresh": 5000,
        "Milk": 3000,
        "Grocery": 4000,
        "Frozen": 1500,
        "Detergents_Paper": 1000,
        "Delicassen": 800,
    }

    inputs = {}

    with colA:
        inputs["Fresh"] = st.number_input("Fresh", min_value=0.0, value=float(defaults["Fresh"]), step=100.0)
        inputs["Milk"] = st.number_input("Milk", min_value=0.0, value=float(defaults["Milk"]), step=100.0)

    with colB:
        inputs["Grocery"] = st.number_input("Grocery", min_value=0.0, value=float(defaults["Grocery"]), step=100.0)
        inputs["Frozen"] = st.number_input("Frozen", min_value=0.0, value=float(defaults["Frozen"]), step=100.0)

    with colC:
        inputs["Detergents_Paper"] = st.number_input("Detergents_Paper", min_value=0.0, value=float(defaults["Detergents_Paper"]), step=100.0)
        inputs["Delicassen"] = st.number_input("Delicassen", min_value=0.0, value=float(defaults["Delicassen"]), step=100.0)

    st.write("**Input Preview**")
    st.dataframe(pd.DataFrame([inputs])[features], use_container_width=True)

    if st.button("Predict Cluster", type="primary"):
        try:
            cluster_id = predict_single(inputs)
            st.success(f"Predicted Cluster: **{cluster_id}**")
            explain_cluster(cluster_id)
        except Exception as e:
            st.error("Prediction failed.")
            st.exception(e)

# =======================
# TAB 2: Batch Prediction
# =======================
with tab2:
    st.subheader("Batch Cluster Prediction (Upload CSV)")
    st.caption(f"CSV must contain these columns: {', '.join(features)}")

    uploaded_csv = st.file_uploader("Upload your CSV file", type=["csv"])

    if uploaded_csv is not None:
        try:
            df = pd.read_csv(uploaded_csv)

            missing = [c for c in features if c not in df.columns]
            if missing:
                st.error(f"Missing required columns: {missing}")
                st.stop()

            df_features = df[features].copy()
            X_scaled = preprocess(df_features)

            preds = kmeans.predict(X_scaled).astype(int)

            df_out = df.copy()
            df_out["Cluster"] = preds

            st.success("Batch prediction completed ✅")
            st.write("Preview (first 20 rows):")
            st.dataframe(df_out.head(20), use_container_width=True)

            # -------- Cluster Summary --------
            st.markdown("### Cluster Summary")

            counts = df_out["Cluster"].value_counts().sort_index()

            # Counts table
            st.write("**Cluster counts:**")
            st.dataframe(counts.rename_axis("Cluster").to_frame("Count"), use_container_width=True)

            # Bar chart (matplotlib)
            fig = plt.figure(figsize=(6, 4))
            plt.bar(counts.index.astype(str), counts.values)
            plt.title("Cluster Distribution")
            plt.xlabel("Cluster")
            plt.ylabel("Number of Customers")
            st.pyplot(fig)

            # Means by cluster
            st.write("**Cluster means (spending columns):**")
            st.dataframe(df_out.groupby("Cluster")[features].mean(), use_container_width=True)

            # -------- Cluster Meaning Panel --------
            with st.expander("What do these clusters represent? (Business Meaning)"):
                for cid in sorted(counts.index.tolist()):
                    st.markdown(f"#### Cluster {cid}")
                    explain_cluster(int(cid))

            # -------- PCA Plot --------
            st.markdown("### 2D Cluster Plot (PCA)")
            st.caption("PCA is used only for visualization (reduces 6D spending space to 2D).")

            pca = PCA(n_components=2, random_state=42)
            X_pca = pca.fit_transform(X_scaled)

            fig2 = plt.figure(figsize=(7, 5))
            for cid in sorted(np.unique(preds)):
                mask = preds == cid
                plt.scatter(X_pca[mask, 0], X_pca[mask, 1], alpha=0.7, label=f"Cluster {cid}")
            plt.title("Customer Segments (PCA View)")
            plt.xlabel("PC1")
            plt.ylabel("PC2")
            plt.legend()
            plt.grid(True)
            st.pyplot(fig2)

            # -------- Download --------
            csv_bytes = df_out.to_csv(index=False).encode("utf-8")
            st.download_button(
                "Download Results CSV",
                data=csv_bytes,
                file_name="wholesale_customers_with_clusters.csv",
                mime="text/csv"
            )

        except Exception as e:
            st.error("Could not process the uploaded file.")
            st.exception(e)

st.markdown("---")
st.caption("Model pipeline: log1p transform → StandardScaler → K-Means clustering.")